{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# FineTuning AraBert for text classification.","metadata":{}},{"cell_type":"markdown","source":"As we saw in previous notebooks both ML and simple LSTM model didn't give us good accuracy. I'll try in this notebook to finetune Arabert which is a model trained on arabic dialects alread: [Arabert](https://github.com/aub-mind/arabert)","metadata":{}},{"cell_type":"markdown","source":"### Importing libraries","metadata":{}},{"cell_type":"code","source":"# !pip install transformers\n!pip install datasets\n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T16:51:21.357378Z","iopub.execute_input":"2022-03-09T16:51:21.357607Z","iopub.status.idle":"2022-03-09T16:51:32.264108Z","shell.execute_reply.started":"2022-03-09T16:51:21.357545Z","shell.execute_reply":"2022-03-09T16:51:32.263178Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset,Dataset,concatenate_datasets\nimport os\nimport time\nimport datetime\nimport random\nimport pandas as pd\nimport numpy as np\n\nfrom transformers import AutoModelForSequenceClassification,AutoConfig,AutoTokenizer\nfrom transformers import AdamW,get_linear_schedule_with_warmup\nfrom sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import AutoConfig, BertForSequenceClassification, AutoTokenizer\nfrom transformers.data.processors import SingleSentenceClassificationProcessor\nfrom transformers import Trainer , TrainingArguments\n\nimport logging\n\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T16:51:32.268044Z","iopub.execute_input":"2022-03-09T16:51:32.268249Z","iopub.status.idle":"2022-03-09T16:51:39.198062Z","shell.execute_reply.started":"2022-03-09T16:51:32.268224Z","shell.execute_reply":"2022-03-09T16:51:39.197395Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## 1- Formatting the data and making dataset using HuggingFace library datasets","metadata":{}},{"cell_type":"markdown","source":"Our data is slightly different from the wanted format, so I'll correct the format.","metadata":{}},{"cell_type":"markdown","source":"### a- Re-formatting the data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/clean-dialect-text/train.csv\",lineterminator='\\n')\nval = pd.read_csv(\"../input/clean-dialect-text/validation.csv\",lineterminator='\\n')\ntest = pd.read_csv(\"../input/clean-dialect-text/test.csv\",lineterminator='\\n')","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-09T16:51:39.199644Z","iopub.execute_input":"2022-03-09T16:51:39.199905Z","iopub.status.idle":"2022-03-09T16:51:41.649045Z","shell.execute_reply.started":"2022-03-09T16:51:39.199870Z","shell.execute_reply":"2022-03-09T16:51:41.648327Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"encoder = LabelEncoder()\nclasses = encoder.fit_transform(train['dialect'])\ntrain['label'] = classes\nval['label'] = encoder.transform(val['dialect'])\n\ntrain.drop(columns=['id','length','dialect'],inplace=True)\ntrain.rename(columns = {'clean_text':'text'}, inplace = True)\n\nval.drop(columns=['id','length','dialect'],inplace=True)\nval.rename(columns = {'clean_text':'text'}, inplace = True)\n\ntrain.to_csv(\"new_train.csv\",index=False)\nval.to_csv(\"new_val.csv\",index=False)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T16:51:41.650421Z","iopub.execute_input":"2022-03-09T16:51:41.650667Z","iopub.status.idle":"2022-03-09T16:51:43.557107Z","shell.execute_reply.started":"2022-03-09T16:51:41.650630Z","shell.execute_reply":"2022-03-09T16:51:43.556322Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"I saved files again to CSV because that is easier to load in the correct dataset splitted to train and test.","metadata":{}},{"cell_type":"code","source":"dataset = load_dataset(\"csv\", data_files={\"train\":\"./new_train.csv\",\"test\":\"./new_val.csv\"}, delimiter=\",\", lineterminator='\\n')","metadata":{"execution":{"iopub.status.busy":"2022-03-09T16:51:43.937761Z","iopub.execute_input":"2022-03-09T16:51:43.938335Z","iopub.status.idle":"2022-03-09T16:51:45.869456Z","shell.execute_reply.started":"2022-03-09T16:51:43.938296Z","shell.execute_reply":"2022-03-09T16:51:45.868677Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"Now we have our dataset, we need to tokenize it to enter the model.","metadata":{}},{"cell_type":"markdown","source":"## 2- Getting the tokenizer, the model and tokenize the data.","metadata":{}},{"cell_type":"code","source":"model_name = 'aubmindlab/bert-base-arabert'\n\nconfig = AutoConfig.from_pretrained(model_name,num_labels=18, output_attentions=True) \ntokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False, do_basic_tokenize=True)\nmodel = BertForSequenceClassification.from_pretrained(model_name,config=config)\n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T16:51:52.549206Z","iopub.execute_input":"2022-03-09T16:51:52.549718Z","iopub.status.idle":"2022-03-09T16:52:14.854039Z","shell.execute_reply.started":"2022-03-09T16:51:52.549679Z","shell.execute_reply":"2022-03-09T16:52:14.853329Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"As mentioned in warning above, the model is not trained and we have to train it on down stream task to use it.","metadata":{}},{"cell_type":"code","source":"def tokenize_function(examples):\n\n    return tokenizer(examples[\"text\"],padding=\"max_length\", truncation=True,return_tensors='pt')\n\n# removed batched = True because I wanted to get pt tensor and when using batched = True we get Python lists.\ntokenized_datasets = dataset.map(tokenize_function)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T17:56:35.105707Z","iopub.execute_input":"2022-03-09T17:56:35.105959Z","iopub.status.idle":"2022-03-09T18:02:04.587133Z","shell.execute_reply.started":"2022-03-09T17:56:35.105929Z","shell.execute_reply":"2022-03-09T18:02:04.586408Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"markdown","source":"Our data is very large, when I used all of it the notebook closed, So I'll just choose a subset to fit into workspace RAM","metadata":{}},{"cell_type":"code","source":"train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(10000))\n\neval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(2000))","metadata":{"execution":{"iopub.status.busy":"2022-03-09T18:16:48.625597Z","iopub.execute_input":"2022-03-09T18:16:48.625858Z","iopub.status.idle":"2022-03-09T18:16:48.790256Z","shell.execute_reply.started":"2022-03-09T18:16:48.625828Z","shell.execute_reply":"2022-03-09T18:16:48.789522Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# just checking different examples to make sure that both datasets have the same length\nlen(eval_dataset[5]['input_ids'])","metadata":{"execution":{"iopub.status.busy":"2022-03-09T09:00:27.606708Z","iopub.execute_input":"2022-03-09T09:00:27.606957Z","iopub.status.idle":"2022-03-09T09:00:27.614754Z","shell.execute_reply.started":"2022-03-09T09:00:27.606930Z","shell.execute_reply":"2022-03-09T09:00:27.613839Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"markdown","source":"## 3- Choosing training arguments","metadata":{}},{"cell_type":"code","source":"training_args = TrainingArguments(\"./train\")\ntraining_args.do_train = True\ntraining_args.evaluate_during_training = True\ntraining_args.adam_epsilon = 1e-5\ntraining_args.learning_rate = 2e-5\ntraining_args.warmup_steps = 0\ntraining_args.per_device_train_batch_size = 16\ntraining_args.per_device_eval_batch_size = 16\ntraining_args.num_train_epochs= 5\ntraining_args.seed = 42","metadata":{"execution":{"iopub.status.busy":"2022-03-09T16:56:10.839802Z","iopub.execute_input":"2022-03-09T16:56:10.840446Z","iopub.status.idle":"2022-03-09T16:56:10.893846Z","shell.execute_reply.started":"2022-03-09T16:56:10.840401Z","shell.execute_reply":"2022-03-09T16:56:10.893153Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"Transformers doesn't have f1_score as a metric, so I used custom function but it didn't work in the end!","metadata":{}},{"cell_type":"code","source":"\n#https://github.com/huggingface/transformers/blob/master/src/transformers/trainer_utils.py\ndef compute_metrics(p): \n    preds = np.argmax(p.predictions, axis=1)\n    assert len(preds) == len(p.label_ids)\n    macro_f1 = f1_score(p.label_ids,preds,average='macro')\n    acc = accuracy_score(p.label_ids,preds)\n    return {\n      'macro_f1' : macro_f1, \n      'accuracy': acc\n   }\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T16:56:13.216805Z","iopub.execute_input":"2022-03-09T16:56:13.217424Z","iopub.status.idle":"2022-03-09T16:56:13.222563Z","shell.execute_reply.started":"2022-03-09T16:56:13.217383Z","shell.execute_reply":"2022-03-09T16:56:13.221882Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(model=model,\n                  args = training_args,\n                  train_dataset = train_dataset,\n                  eval_dataset = eval_dataset,\n                  compute_metrics = compute_metrics)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T16:56:17.984659Z","iopub.execute_input":"2022-03-09T16:56:17.984920Z","iopub.status.idle":"2022-03-09T16:56:23.295681Z","shell.execute_reply.started":"2022-03-09T16:56:17.984889Z","shell.execute_reply":"2022-03-09T16:56:23.294666Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T16:56:26.356219Z","iopub.execute_input":"2022-03-09T16:56:26.357011Z","iopub.status.idle":"2022-03-09T17:43:04.886641Z","shell.execute_reply.started":"2022-03-09T16:56:26.356968Z","shell.execute_reply":"2022-03-09T17:43:04.885931Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"#model.eval()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T18:24:32.974177Z","iopub.execute_input":"2022-03-09T18:24:32.974840Z","iopub.status.idle":"2022-03-09T18:24:32.979494Z","shell.execute_reply.started":"2022-03-09T18:24:32.974802Z","shell.execute_reply":"2022-03-09T18:24:32.978575Z"},"trusted":true},"execution_count":51,"outputs":[]},{"cell_type":"markdown","source":"## 4 - Evaluation","metadata":{}},{"cell_type":"markdown","source":"As I mentioned above I faced a problem with the function so I made another one, very basic without creativity or optimization but doing the job. ","metadata":{}},{"cell_type":"code","source":"def eval(data):\n    labels = []\n    preds = []\n    # range is 20000 also because of RAM\n    for i in range(20000):\n        #print(example)\n        example = data[i]\n        text = example['text']\n        labels.append(example['label'])\n        tokens = tokenizer([text],max_length=512,return_tensors='pt').to('cuda')\n        out = model(**tokens)\n        l = torch.argmax(out['logits'][0]).item()\n        preds.append(l)\n    macro_f1 = f1_score(labels,preds,average='macro')\n    return macro_f1","metadata":{"execution":{"iopub.status.busy":"2022-03-09T19:18:21.194438Z","iopub.execute_input":"2022-03-09T19:18:21.195125Z","iopub.status.idle":"2022-03-09T19:18:21.204207Z","shell.execute_reply.started":"2022-03-09T19:18:21.195085Z","shell.execute_reply":"2022-03-09T19:18:21.203429Z"},"trusted":true},"execution_count":119,"outputs":[]},{"cell_type":"code","source":"eval(dataset['test'])","metadata":{"execution":{"iopub.status.busy":"2022-03-09T19:18:23.448598Z","iopub.execute_input":"2022-03-09T19:18:23.449315Z","iopub.status.idle":"2022-03-09T19:21:48.608551Z","shell.execute_reply.started":"2022-03-09T19:18:23.449278Z","shell.execute_reply":"2022-03-09T19:21:48.607853Z"},"trusted":true},"execution_count":120,"outputs":[]},{"cell_type":"markdown","source":"As we can see it's worst than machine learning model and basic LSTM, but I'm sure that the problem within my approach. This notebook needs more work.","metadata":{}},{"cell_type":"markdown","source":"## 5- Saving the model","metadata":{}},{"cell_type":"code","source":"model.save_pretrained(\"model.bin\")","metadata":{"execution":{"iopub.status.busy":"2022-03-09T18:58:15.325131Z","iopub.execute_input":"2022-03-09T18:58:15.325420Z","iopub.status.idle":"2022-03-09T18:58:16.357040Z","shell.execute_reply.started":"2022-03-09T18:58:15.325384Z","shell.execute_reply":"2022-03-09T18:58:16.356260Z"},"trusted":true},"execution_count":114,"outputs":[]}]}